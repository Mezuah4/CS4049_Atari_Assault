{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Attempt_HYBRID.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Setup"
      ],
      "metadata": {
        "id": "SegabyNrs6ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqJ8hL0ks2WY"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "%pip install -U tf-agents pyvirtualdisplay\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[box2d,atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Maths\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "\n",
        "# Virtual Display\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# Common \n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Saving\n",
        "import pickle\n",
        "\n",
        "# Other\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "IDJL_8xGtHof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "PROJECT_ROOT_DIR = 'drive/MyDrive/ML/Keras_Version/Hybrid'\n",
        "\n",
        "# SEEDS\n",
        "np.random.seed(69)\n",
        "tf.random.set_seed(420)"
      ],
      "metadata": {
        "id": "BCqf3K09ttMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6BywEzZAu4Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "amtJd48Nv643"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing\n",
        "def preprocess_observation(observation):\n",
        "\n",
        "    # Slice Top Off\n",
        "    img = observation[14:210:2, ::2]\n",
        "\n",
        "    # Grey Scale\n",
        "    img = img.mean(axis=2)\n",
        "    img = (img - 128).astype(np.float32)\n",
        "\n",
        "    return img.reshape(98, 80, 1)"
      ],
      "metadata": {
        "id": "-zS97UFZv_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon Greedy Policy\n",
        "def epsilon_greedy_policy_hybrid(ram, img, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(np.array([img, ram]))\n",
        "        return np.argmax(Q_values[0])"
      ],
      "metadata": {
        "id": "lWbEuZS7wK88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Experiences\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(REPLAY_MEMORY), size=batch_size)\n",
        "    batch = [REPLAY_MEMORY[index] for index in indices]\n",
        "    ram, img, actions, rewards, next_state_img, next_state_ram, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(7)]\n",
        "    return ram, img, actions, rewards, next_state_img, next_state_ram, dones"
      ],
      "metadata": {
        "id": "FjzaD2dqzD0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Iteration\n",
        "def playHybrid(env, ram, img, epsilon, frame_skip):\n",
        "\n",
        "    # Get Action\n",
        "    action = epsilon_greedy_policy_hybrid(ram, img, epsilon)\n",
        "\n",
        "    # Do Action frame_skip Times\n",
        "    iter_reward = 0\n",
        "    for frame in range(frame_skip):\n",
        "        next_state_img, reward, done, info = env.step(action)\n",
        "        iter_reward += reward\n",
        "\n",
        "\n",
        "    next_state_img = preprocess_observation(next_state_img)\n",
        "    next_state_ram = env.ale.getRAM()\n",
        "\n",
        "    # Add Last Frame to Buffer\n",
        "    REPLAY_MEMORY.append((ram, img, action, iter_reward, next_state_img, next_state_ram, done))\n",
        "    return next_state_img, next_state_ram, iter_reward, done, info"
      ],
      "metadata": {
        "id": "vaBBiGujzyJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "batch_size = 32\n",
        "discount_rate = 0.99\n",
        "learning_rate = 0.00025\n",
        "momentum = 0.95\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "# Train from Memory\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    ram, img, actions, rewards, next_state_img, next_state_ram, dones = experiences\n",
        "    next_Q_values = model.predict(np.array(next_state_img, next_state_ram))\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values).reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model([next_state_img, next_state_ram])\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "bes2g-ud0cV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning with Frames"
      ],
      "metadata": {
        "id": "ecReBy4Pv8s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environment\n",
        "keras.backend.clear_session()\n",
        "\n",
        "env = gym.make(\"AssaultNoFrameskip-v4\")\n",
        "\n",
        "env.reset()\n",
        "exampleRamInput = env.ale.getRAM()\n",
        "\n",
        "input_shape_img = (98, 80, 1)\n",
        "input_shape_ram = (None, 128)\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "initializer = keras.initializers.VarianceScaling()\n",
        "\n",
        "def buildModel():\n",
        "    imgMod = keras.models.Sequential()\n",
        "    imgMod.add(keras.layers.Conv2D(filters=32, kernel_size=8, strides=4, padding=\"same\", activation=\"relu\", kernel_initializer=initializer, input_shape=input_shape_img))\n",
        "    imgMod.add(keras.layers.Conv2D(filters=16, kernel_size=4, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=initializer))\n",
        "    imgMod.add(keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=initializer))\n",
        "    imgMod.add(keras.layers.Flatten())\n",
        "\n",
        "    ramMod = keras.models.Sequential()\n",
        "    ramInputLayer = keras.layers.Input(128)\n",
        "    ramMod.add(ramInputLayer)\n",
        "    ramMod.add(keras.layers.Dense(128, activation=\"relu\", kernel_initializer=initializer))\n",
        "\n",
        "    combMod = keras.models.Sequential()\n",
        "    combMod.add(keras.layers.Concatenate([imgMod.output, ramMod.output]))\n",
        "\n",
        "    combMod.add(keras.layers.Dense(units=640, activation=\"relu\", kernel_initializer=initializer))\n",
        "    combMod.add(keras.layers.Dense(units=512, activation=\"relu\", kernel_initializer=initializer))\n",
        "    combMod.add(keras.layers.Dense(n_outputs, activation=\"relu\", kernel_initializer=initializer))\n",
        "    return combMod\n",
        "\n",
        "model = buildModel()"
      ],
      "metadata": {
        "id": "vqYJIULICTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Convolutional Model\n",
        "\n",
        "# CONFIG\n",
        "env.seed(710)\n",
        "REPLAY_MEMORY = deque(maxlen=100)\n",
        "FRAMESKIP = 4\n",
        "EPISODES = 1000\n",
        "STEPS = 1000000\n",
        "WARMUP = 35\n",
        "\n",
        "\n",
        "episode_rewards = []\n",
        "best_score = 0\n",
        "\n",
        "step = 0\n",
        "for episode in range(EPISODES):\n",
        "\n",
        "    next_step_img = preprocess_observation(env.reset())\n",
        "    ram = env.ale.getRAM()\n",
        "    \n",
        "    episode_rewards.append(0)\n",
        "    while True:\n",
        "        step += 1\n",
        "        epsilon = max(1 - episode / EPISODES, 0.01)\n",
        "\n",
        "        next_step_img, next_step_ram, reward, done, info = playHybrid(env, ram, next_step_img, epsilon, FRAMESKIP)\n",
        "\n",
        "        episode_rewards[episode] += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}, current_reward: {}\".format(episode, step + 1, epsilon, episode_rewards[episode]), end=\"\")\n",
        "    \n",
        "    # if episode > WARMUP:\n",
        "    training_step(batch_size)\n",
        "\n",
        "np.savetxt(PROJECT_ROOT_DIR + \"/RESULTS/\" + \"res.csv\", np.asarray(episode_rewards), delimiter=\",\")"
      ],
      "metadata": {
        "id": "pMPd26Qpy3On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Results\n",
        "\n",
        "EPISODES_PER_EPOCH = 9\n",
        "\n",
        "average_rewards = [np.mean(episode_rewards[i - EPISODES_PER_EPOCH : i]) for i in range(len(episode_rewards), EPISODES_PER_EPOCH, -1)]\n",
        "\n",
        "bins = [x/10 for x in range(len(average_rewards))] \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(bins, average_rewards)\n",
        "\n",
        "plt.xticks(list(range(0, 18)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Reward Over Last 10 Episodes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KG9JTuKg22Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        ""
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ymEQUPstjjiM"
      }
    }
  ]
}