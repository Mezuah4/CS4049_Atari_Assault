{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Attempt_FRAMES.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "authorship_tag": "ABX9TyOzGyam+ntXEt8u4HhMCk8Q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Setup"
      ],
      "metadata": {
        "id": "SegabyNrs6ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqJ8hL0ks2WY"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "%pip install -U tf-agents pyvirtualdisplay\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[box2d,atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Maths\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "\n",
        "# Virtual Display\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# Common\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Saving\n",
        "import pickle\n",
        "\n",
        "# Other\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "IDJL_8xGtHof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "PROJECT_ROOT_DIR = './drive/MyDrive/ML/FRAMES/'\n",
        "\n",
        "# SEEDS\n",
        "np.random.seed(69)\n",
        "tf.random.set_seed(420)"
      ],
      "metadata": {
        "id": "BCqf3K09ttMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6BywEzZAu4Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "amtJd48Nv643"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing\n",
        "def preprocess_observation(observation):\n",
        "\n",
        "    # Slice Top Off\n",
        "    img = observation[14:210:2, ::2]\n",
        "\n",
        "    # Grey Scale\n",
        "    img = img.mean(axis=2)\n",
        "    img = (img - 128).astype(np.float32)\n",
        "\n",
        "    return img.reshape(98, 80, 1)"
      ],
      "metadata": {
        "id": "-zS97UFZv_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon Greedy Policy\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict(np.array([state]))\n",
        "        return np.argmax(Q_values[0])"
      ],
      "metadata": {
        "id": "lWbEuZS7wK88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Experiences\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(REPLAY_MEMORY), size=batch_size)\n",
        "    batch = [REPLAY_MEMORY[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "metadata": {
        "id": "FjzaD2dqzD0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Iteration\n",
        "def play(env, state, epsilon, frame_skip):\n",
        "\n",
        "    # Get Action\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "\n",
        "    # Do Action frame_skip Times\n",
        "    iter_reward = 0\n",
        "    for frame in range(frame_skip):\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        iter_reward += reward\n",
        "\n",
        "    next_state = preprocess_observation(next_state)\n",
        "\n",
        "    # Add Last Frame to Buffer\n",
        "    REPLAY_MEMORY.append((state, action, iter_reward, next_state, done))\n",
        "    return next_state, iter_reward, done, info"
      ],
      "metadata": {
        "id": "vaBBiGujzyJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "batch_size = 32\n",
        "discount_rate = 0.99\n",
        "learning_rate = 0.00025\n",
        "momentum = 0.95\n",
        "optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "# Train from Memory\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values).reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "bes2g-ud0cV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning with Frames"
      ],
      "metadata": {
        "id": "ecReBy4Pv8s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environment\n",
        "keras.backend.clear_session()\n",
        "\n",
        "env = gym.make(\"AssaultNoFrameskip-v4\")\n",
        "input_shape = (98, 80, 1)\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "initializer = keras.initializers.VarianceScaling()\n",
        "\n",
        "# Create Model\n",
        "model = keras.models.Sequential([\n",
        "                               keras.layers.Conv2D(filters=32, kernel_size=8, strides=4,\n",
        "                                                   padding=\"same\", activation=\"relu\",\n",
        "                                                   kernel_initializer=initializer,\n",
        "                                                   input_shape=input_shape),\n",
        "                               keras.layers.Conv2D(filters=16, kernel_size=4, strides=2,\n",
        "                                                   padding=\"same\", activation=\"relu\",\n",
        "                                                   kernel_initializer=initializer),\n",
        "                               keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
        "                                                   padding=\"same\", activation=\"relu\",\n",
        "                                                   kernel_initializer=initializer),\n",
        "                               keras.layers.Flatten(),\n",
        "                               keras.layers.Dense(units=512, activation=\"relu\",\n",
        "                                                  kernel_initializer=initializer),\n",
        "                               keras.layers.Dense(n_outputs, activation=\"relu\",\n",
        "                                                  kernel_initializer=initializer)\n",
        "])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "vqYJIULICTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Convolutional Model\n",
        "\n",
        "# CONFIG\n",
        "env.seed(710)\n",
        "\n",
        "REPLAY_MEMORY = deque(maxlen=2000)\n",
        "\n",
        "FRAMESKIP = 4\n",
        "START_EPISODE = 0\n",
        "EPISODES = 1750\n",
        "\n",
        "EPSILON = 1\n",
        "EPSILON_MAX = 1\n",
        "DECAY = 0.99884936993651\n",
        "\n",
        "# Load Existing Model (if it exists)\n",
        "if os.path.isfile(\"./drive/MyDrive/ML/FRAMES/WEIGHTS/checkpoint.ckpt.index\"):\n",
        "    model.load_weights(\"./drive/MyDrive/ML/FRAMES/WEIGHTS/checkpoint.ckpt\")\n",
        "    print(\"Successfully Loaded Previous Weights\")\n",
        "else:\n",
        "    print(\"Weights not loaded.\")\n",
        "\n",
        "episode_rewards = []\n",
        "best_score = 0\n",
        "step = 0\n",
        "\n",
        "# Load Previous Values\n",
        "try:\n",
        "    with open(PROJECT_ROOT_DIR + \"VARIABLES/\" + \"vars.pickle\", 'rb') as v:\n",
        "        dic = pickle.load(v)\n",
        "        START_EPISODE = dic[\"episode\"]\n",
        "        EPSILON = dic[\"eps\"]\n",
        "        episode_rewards = dic[\"rewards\"]\n",
        "        step = dic[\"step\"]\n",
        "        print(\"Successfully Loaded Previous Variables\")\n",
        "except:\n",
        "    print(\"No Files Loaded\")\n",
        "\n",
        "for episode in range(START_EPISODE, EPISODES):\n",
        "    \n",
        "    # Reset Env\n",
        "    obs = preprocess_observation(env.reset())\n",
        "    episode_rewards.append(0)\n",
        "\n",
        "    # Decay Epsilon\n",
        "    EPSILON = EPSILON_MAX * (DECAY ** episode)\n",
        "\n",
        "    # Each Episode\n",
        "    while True:\n",
        "\n",
        "        step += 1\n",
        "        obs, reward, done, info = play(env, obs, EPSILON, FRAMESKIP)\n",
        "\n",
        "        episode_rewards[episode] += reward\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            \n",
        "            # Save Model\n",
        "            model.save_weights(PROJECT_ROOT_DIR + \"WEIGHTS/checkpoint.ckpt\")\n",
        "\n",
        "            # Save Variables\n",
        "            dic = {\"rewards\":episode_rewards, \"eps\":EPSILON, \"step\":step, \"episode\":episode}\n",
        "            with open(PROJECT_ROOT_DIR + \"VARIABLES/\" + \"vars.pickle\", \"wb\") as v:\n",
        "                pickle.dump(dic, v)\n",
        "\n",
        "        if done:\n",
        "            # Episode is Finished\n",
        "            break\n",
        " \n",
        "        print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}, current_reward: {}, percentage: {:.2f}\".format(episode, step + 1, EPSILON, episode_rewards[episode], episode/EPISODES*100), end=\"\")\n",
        "    \n",
        "    # Train Model on Buffer Sample \n",
        "    training_step(batch_size)\n",
        "\n",
        "np.savetxt(PROJECT_ROOT_DIR + \"RESULTS/\" + \"res.csv\", np.asarray(episode_rewards), delimiter=\",\")"
      ],
      "metadata": {
        "id": "pMPd26Qpy3On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Results\n",
        "\n",
        "EPISODES_PER_EPOCH = 9\n",
        "\n",
        "average_rewards = []\n",
        "for i in range(0, len(episode_rewards)):\n",
        "    if i == 0:\n",
        "        average_rewards.append(episode_rewards[i])\n",
        "    elif i < EPISODES_PER_EPOCH:\n",
        "        average_rewards.append(np.mean(episode_rewards[:i + 1]))\n",
        "    else:\n",
        "        average_rewards.append(np.mean(episode_rewards[i - EPISODES_PER_EPOCH: i]))\n",
        "\n",
        "\n",
        "bins = [x/10 for x in range(len(average_rewards))] \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(bins, average_rewards)\n",
        "\n",
        "# plt.xticks(list(range(0, 18)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Reward Over Last 10 Episodes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KG9JTuKg22Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Results\n",
        "\n",
        "EPISODES_PER_EPOCH = 99\n",
        "\n",
        "average_rewards = []\n",
        "for i in range(0, len(episode_rewards)):\n",
        "    if i == 0:\n",
        "        average_rewards.append(episode_rewards[i])\n",
        "    elif i < EPISODES_PER_EPOCH:\n",
        "        average_rewards.append(np.mean(episode_rewards[:i + 1]))\n",
        "    else:\n",
        "        average_rewards.append(np.mean(episode_rewards[i - EPISODES_PER_EPOCH: i]))\n",
        "\n",
        "\n",
        "bins = [x/100 for x in range(len(average_rewards))] \n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(bins, average_rewards)\n",
        "\n",
        "# plt.xticks(list(range(0, 18)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Mean Reward Over Last 100 Episodes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z0e94jzsrgzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rewards = []\n",
        "\n",
        "env.seed(6969)\n",
        "\n",
        "# Testing\n",
        "for episode in range(0, 10):\n",
        "\n",
        "    # Reset Env\n",
        "    obs = preprocess_observation(env.reset())\n",
        "    test_rewards.append(0)\n",
        "\n",
        "    # Each Episode\n",
        "    while True:\n",
        "\n",
        "        step += 1\n",
        "        next_step_img, reward, done, info = play(env, obs, EPSILON, FRAMESKIP)\n",
        "        test_rewards[episode] += reward\n",
        "\n",
        "        if done:\n",
        "          # Episode finished\n",
        "            break\n",
        "\n",
        "    print(\"\\nEpisode: {}, Reward: {}\".format(episode + 1, test_rewards[episode]), end=\"\")"
      ],
      "metadata": {
        "id": "9e1SQB9vrEmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(PROJECT_ROOT_DIR + \"RESULTS/\" + \"frames_test.csv\", np.asarray(test_rewards), delimiter=\",\")"
      ],
      "metadata": {
        "id": "0k2pMllOWyak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise One Episode\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "\n",
        "# Animation Helpers\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "\n",
        "# Reset Env\n",
        "next_step_img = preprocess_observation(env.reset())\n",
        "\n",
        "frames = []\n",
        "\n",
        "# Each Episode\n",
        "while True:\n",
        "\n",
        "    step += 1\n",
        "    next_step_img, reward, done, info = play(env, next_step_img, EPSILON, FRAMESKIP)\n",
        "\n",
        "    img = env.render(mode=\"rgb_array\")\n",
        "    frames.append(img)\n",
        "\n",
        "    if done:\n",
        "        # Episode finished\n",
        "        break\n",
        "\n",
        "plot_animation(frames)"
      ],
      "metadata": {
        "id": "S1HV3F19W1wC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}