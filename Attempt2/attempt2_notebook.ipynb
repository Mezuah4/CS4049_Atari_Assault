{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# For deep neural networks\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tf_slim as slim\n",
    "\n",
    "# For data representation\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# For handling files\n",
    "import os\n",
    "\n",
    "# For plotting graphs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenAI Gym\n",
    "import gym\n",
    "# I have installed pyglet-1.5.11 for it work with BigSur\n",
    "\n",
    "# Cael Defined Tools\n",
    "from Tools.preprocessing import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Screen only agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(7)\n",
      "Observation space:  Box(250, 160, 3)\n",
      "{'ale.lives': 4}\n"
     ]
    }
   ],
   "source": [
    "# Create Environment\n",
    "env = gym.make(\"Assault-v0\")\n",
    "\n",
    "# Get environment information\n",
    "height, width, channels = env.observation_space.shape   # Image shape\n",
    "num_actions = env.action_space.n    # Number of actions\n",
    "\n",
    "# Describe Action & Observation Space\n",
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "\n",
    "env.reset()\n",
    "next_obs, reward, done, info = env.step(0)\n",
    "print(info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initilise network parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "network_input_height = 108\n",
    "network_input_width = 80\n",
    "network_input_channels = 1\n",
    "conv_n_maps = [32, 64, 64]\n",
    "conv_kernel_sizes = [(8,8), (4,4), (3,3)]\n",
    "conv_strides = [4, 2, 1]\n",
    "conv_paddings = [\"SAME\"] * 3\n",
    "conv_activation = [tf.nn.relu] * 3\n",
    "n_hidden_in = 64 * 14 * 10  # conv3 has 64 maps of 11x10 each\n",
    "n_hidden = 512\n",
    "hidden_activation = tf.nn.relu\n",
    "n_outputs = env.action_space.n  # 9 discrete actions are available\n",
    "initializer = tf.variance_scaling_initializer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def q_network(X_state, name):\n",
    "    prev_layer = X_state / 128.0 # scale pixel intensities to the [-1.0, 1.0] range.\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        for n_maps, kernel_size, strides, padding, activation in zip(\n",
    "                conv_n_maps, conv_kernel_sizes, conv_strides,\n",
    "                conv_paddings, conv_activation):\n",
    "            prev_layer = tf.layers.conv2d(\n",
    "                prev_layer, filters=n_maps, kernel_size=kernel_size,\n",
    "                strides=strides, padding=padding, activation=activation,\n",
    "                kernel_initializer=initializer)\n",
    "\n",
    "        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])\n",
    "        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,\n",
    "                                 activation=hidden_activation,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(hidden, n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "X_state = tf.placeholder(tf.float32, shape=[None, network_input_height, network_input_width,\n",
    "                                            network_input_channels])\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "momentum = 0.95\n",
    "\n",
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),\n",
    "                            axis=1, keepdims=True)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "\n",
    "    def append(self, data):\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "\n",
    "    def sample(self, batch_size, with_replacement=True):\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size=batch_size) # faster\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cael Milne\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "replay_memory_size = 500000\n",
    "replay_memory = ReplayMemory(replay_memory_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 2000000\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "n_steps = 4000000  # total number of training steps\n",
    "training_start = 10000  # start training after 10,000 game iterations\n",
    "training_interval = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 10000  # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate = 0.99\n",
    "skip_start = 90  # Skip the start of every game (it's just waiting time).\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "checkpoint_path = \"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 382968\tTraining step 93242/4000000 (2.3)%\tLoss 0.000570\tMean Max-Q 0.459332   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\CAELMI~1\\AppData\\Local\\Temp/ipykernel_9920/369355243.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     51\u001B[0m         \u001B[1;31m# Train the online DQN\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m         _, loss_val = sess.run([training_op, loss], feed_dict={\n\u001B[1;32m---> 53\u001B[1;33m             X_state: X_state_val, X_action: X_action_val, y: y_val})\n\u001B[0m\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[1;31m# Regularly copy the online DQN to the target DQN\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    956\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    957\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[1;32m--> 958\u001B[1;33m                          run_metadata_ptr)\n\u001B[0m\u001B[0;32m    959\u001B[0m       \u001B[1;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    960\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1179\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1180\u001B[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[1;32m-> 1181\u001B[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001B[0m\u001B[0;32m   1182\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1183\u001B[0m       \u001B[0mresults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_run\u001B[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1357\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1358\u001B[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[1;32m-> 1359\u001B[1;33m                            run_metadata)\n\u001B[0m\u001B[0;32m   1360\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_prun_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeeds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetches\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_do_call\u001B[1;34m(self, fn, *args)\u001B[0m\n\u001B[0;32m   1363\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_do_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1365\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1366\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1367\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_text\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[0;32m   1348\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1349\u001B[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001B[1;32m-> 1350\u001B[1;33m                                       target_list, run_metadata)\n\u001B[0m\u001B[0;32m   1351\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1352\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_prun_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[0;32m   1441\u001B[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001B[0;32m   1442\u001B[0m                                             \u001B[0mfetch_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1443\u001B[1;33m                                             run_metadata)\n\u001B[0m\u001B[0;32m   1444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1445\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_tf_sessionprun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_path + \".index\"):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "        copy_online_to_target.run()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        print(\"\\rIteration {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:5f}\\tMean Max-Q {:5f}   \".format(\n",
    "            iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\")\n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            for skip in range(skip_start): # skip the start of each game\n",
    "                obs, reward, done, info = env.step(0)\n",
    "            state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "\n",
    "        # Let's memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # Compute statistics for tracking progress (not shown in the book)\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # only train after warmup period and at regular intervals\n",
    "\n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = (\n",
    "            sample_memories(batch_size))\n",
    "        next_q_values = target_q_values.eval(\n",
    "            feed_dict={X_state: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "\n",
    "        # Train the online DQN\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "        # And save regularly\n",
    "        if step % save_steps == 0:\n",
    "            saver.save(sess, checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frames = []\n",
    "n_max_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        state = preprocess_observation(obs)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "\n",
    "        if done:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}