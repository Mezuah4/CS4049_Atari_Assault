{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Attempt.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2j+GyZbyAlZZ9K+HQKORf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Setup"
      ],
      "metadata": {
        "id": "SegabyNrs6ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqJ8hL0ks2WY"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "%pip install -U tf-agents pyvirtualdisplay\n",
        "%pip install -U gym>=0.21.0\n",
        "%pip install -U gym[box2d,atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Maths\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "\n",
        "# Virtual Display\n",
        "import pyvirtualdisplay\n",
        "\n",
        "# Common\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Other\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "IDJL_8xGtHof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "PROJECT_ROOT_DIR = 'drive/MyDrive/ML/Keras_Version'\n",
        "\n",
        "# SEEDS\n",
        "np.random.seed(69)\n",
        "tf.random.set_seed(420)"
      ],
      "metadata": {
        "id": "BCqf3K09ttMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6BywEzZAu4Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "amtJd48Nv643"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Processing\n",
        "def preprocess_observation(observation):\n",
        "\n",
        "    # Slice Top Off\n",
        "    img = observation[14:210:2, ::2]\n",
        "\n",
        "    # Grey Scale\n",
        "    img = img.mean(axis=2)\n",
        "    img = (img - 128).astype(np.float32)\n",
        "\n",
        "    return img.reshape(98, 80, 1)"
      ],
      "metadata": {
        "id": "-zS97UFZv_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epsilon Greedy Policy\n",
        "def epsilon_greedy_policy(state, epsilon=0):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        Q_values = model.predict_step(np.array([state]))\n",
        "        return np.argmax(Q_values[0])"
      ],
      "metadata": {
        "id": "lWbEuZS7wK88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Experiences\n",
        "def sample_experiences(batch_size):\n",
        "    indices = np.random.randint(len(REPLAY_MEMORY), size=batch_size)\n",
        "    batch = [REPLAY_MEMORY[index] for index in indices]\n",
        "    states, actions, rewards, next_states, dones = [np.array([experience[field_index] for experience in batch]) for field_index in range(5)]\n",
        "    return states, actions, rewards, next_states, dones"
      ],
      "metadata": {
        "id": "FjzaD2dqzD0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play One Step\n",
        "def play_one_step(env, state, epsilon):\n",
        "    action = epsilon_greedy_policy(state, epsilon)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = preprocess_observation(next_state)\n",
        "    REPLAY_MEMORY.append((state, action, reward, next_state, done))\n",
        "    return next_state, reward, done, info"
      ],
      "metadata": {
        "id": "vaBBiGujzyJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "batch_size = 50\n",
        "discount_rate = 0.99\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "def training_step(batch_size):\n",
        "    experiences = sample_experiences(batch_size)\n",
        "    states, actions, rewards, next_states, dones = experiences\n",
        "    next_Q_values = model.predict(next_states)\n",
        "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values).reshape(-1, 1)\n",
        "    mask = tf.one_hot(actions, n_outputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        all_Q_values = model(states)\n",
        "        Q_values = tf.reduce_sum(all_Q_values * mask, axis = 1, keepdims=True)\n",
        "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "bes2g-ud0cV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning with Frames"
      ],
      "metadata": {
        "id": "ecReBy4Pv8s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Environment\n",
        "keras.backend.clear_session()\n",
        "\n",
        "env = gym.make(\"Assault-v0\")\n",
        "input_shape = (98, 80, 1)\n",
        "n_outputs = env.action_space.n\n",
        "\n",
        "# Create Model\n",
        "model = keras.models.Sequential([\n",
        "                               keras.layers.Conv2D(filters=32, kernel_size=8, activation=\"relu\", input_shape=input_shape),\n",
        "                               keras.layers.Conv2D(filters=16, kernel_size=4, activation=\"relu\"),\n",
        "                               keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "                               keras.layers.Flatten(),\n",
        "                               keras.layers.Dense(n_outputs, activation=\"relu\")\n",
        "])\n",
        "\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "vqYJIULICTNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Convolutional Model\n",
        "\n",
        "# CONFIG\n",
        "env.seed(710)\n",
        "REPLAY_MEMORY = deque(maxlen=100)\n",
        "EPISODES = 1000\n",
        "WARMUP = 35\n",
        "\n",
        "\n",
        "episode_rewards = []\n",
        "best_score = 0\n",
        "step = 0\n",
        "\n",
        "for episode in range(EPISODES):\n",
        "    \n",
        "    obs = preprocess_observation(env.reset())\n",
        "    \n",
        "    episode_rewards.append(0)\n",
        "    while True:\n",
        "        step += 1\n",
        "        epsilon = max(1 - episode / EPISODES, 0.01)\n",
        "        \n",
        "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
        "\n",
        "        episode_rewards[episode] += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}, current_reward: {}\".format(episode, step + 1, epsilon, episode_rewards[episode]), end=\"\")\n",
        "    \n",
        "    # if episode > WARMUP:\n",
        "    training_step(batch_size)"
      ],
      "metadata": {
        "id": "pMPd26Qpy3On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Results\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(episode_rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Sum of Rewards\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KG9JTuKg22Dm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}